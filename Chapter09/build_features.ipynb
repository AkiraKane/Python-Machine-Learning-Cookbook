{
 "metadata": {},
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import argparse\n",
      "import pickle as pickle\n",
      "import json\n",
      "\n",
      "import cv2\n",
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "from star_detector import StarFeatureDetector\n",
      "\n",
      "def build_arg_parser():\n",
      "    parser = argparse.ArgumentParser(description='Extract features from a given \\\n",
      "            set of images')\n",
      "\n",
      "    parser.add_argument(\"--data-folder\", dest=\"data_folder\", required=True, \n",
      "            help=\"Folder containing the training images organized in subfolders\")\n",
      "    parser.add_argument(\"--codebook-file\", dest='codebook_file', required=True,\n",
      "            help=\"Output file where the codebook will be stored\")\n",
      "    parser.add_argument(\"--feature-map-file\", dest='feature_map_file', required=True,\n",
      "            help=\"Output file where the feature map will be stored\")\n",
      "    parser.add_argument(\"--scaling-size\", dest=\"scaling_size\", type=int, \n",
      "            default=200, help=\"Scales the longer dimension of the image down \\\n",
      "                    to this size.\")\n",
      "\n",
      "    return parser\n",
      "\n",
      "def load_training_data(input_folder):\n",
      "    training_data = []\n",
      "\n",
      "    if not os.path.isdir(input_folder):\n",
      "        raise IOError(\"The folder \" + input_folder + \" doesn't exist\")\n",
      "        \n",
      "    for root, dirs, files in os.walk(input_folder):\n",
      "        for filename in (x for x in files if x.endswith('.jpg')):\n",
      "            filepath = os.path.join(root, filename)\n",
      "            object_class = filepath.split('/')[-2]\n",
      "            training_data.append({'object_class': object_class, \n",
      "                'image_path': filepath})\n",
      "                    \n",
      "    return training_data\n",
      "\n",
      "class FeatureBuilder(object):\n",
      "    def extract_features(self, img):\n",
      "        keypoints = StarFeatureDetector().detect(img)\n",
      "        keypoints, feature_vectors = compute_sift_features(img, keypoints)\n",
      "        return feature_vectors\n",
      "\n",
      "    def get_codewords(self, input_map, scaling_size, max_samples=12):\n",
      "        keypoints_all = []\n",
      "        \n",
      "        count = 0\n",
      "        cur_class = ''\n",
      "        for item in input_map:\n",
      "            if count >= max_samples:\n",
      "                if cur_class != item['object_class']:\n",
      "                    count = 0\n",
      "                else:\n",
      "                    continue\n",
      "\n",
      "            count += 1\n",
      "\n",
      "            if count == max_samples:\n",
      "                print(\"Built centroids for\", item['object_class'])\n",
      "\n",
      "            cur_class = item['object_class']\n",
      "            img = cv2.imread(item['image_path'])\n",
      "            img = resize_image(img, scaling_size)\n",
      "\n",
      "            num_dims = 128\n",
      "            feature_vectors = self.extract_features(img)\n",
      "            keypoints_all.extend(feature_vectors) \n",
      "\n",
      "        kmeans, centroids = BagOfWords().cluster(keypoints_all)\n",
      "        return kmeans, centroids\n",
      "\n",
      "class BagOfWords(object):\n",
      "    def __init__(self, num_clusters=32):\n",
      "        self.num_dims = 128\n",
      "        self.num_clusters = num_clusters\n",
      "        self.num_retries = 10\n",
      "\n",
      "    def cluster(self, datapoints):\n",
      "        kmeans = KMeans(self.num_clusters, \n",
      "                        n_init=max(self.num_retries, 1),\n",
      "                        max_iter=10, tol=1.0)\n",
      "\n",
      "        res = kmeans.fit(datapoints)\n",
      "        centroids = res.cluster_centers_\n",
      "        return kmeans, centroids\n",
      "\n",
      "    def normalize(self, input_data):\n",
      "        sum_input = np.sum(input_data)\n",
      "\n",
      "        if sum_input > 0:\n",
      "            return input_data / sum_input\n",
      "        else:\n",
      "            return input_data\n",
      "\n",
      "    def construct_feature(self, img, kmeans, centroids):\n",
      "        keypoints = StarFeatureDetector().detect(img)\n",
      "        keypoints, feature_vectors = compute_sift_features(img, keypoints)\n",
      "        labels = kmeans.predict(feature_vectors)\n",
      "        feature_vector = np.zeros(self.num_clusters)\n",
      "\n",
      "        for i, item in enumerate(feature_vectors):\n",
      "            feature_vector[labels[i]] += 1\n",
      "\n",
      "        feature_vector_img = np.reshape(feature_vector, \n",
      "                ((1, feature_vector.shape[0])))\n",
      "        return self.normalize(feature_vector_img)\n",
      "\n",
      "# Extract features from the input images and \n",
      "# map them to the corresponding object classes\n",
      "def get_feature_map(input_map, kmeans, centroids, scaling_size):\n",
      "    feature_map = []\n",
      "     \n",
      "    for item in input_map:\n",
      "        temp_dict = {}\n",
      "        temp_dict['object_class'] = item['object_class']\n",
      "    \n",
      "        print(\"Extracting features for\", item['image_path'])\n",
      "        img = cv2.imread(item['image_path'])\n",
      "        img = resize_image(img, scaling_size)\n",
      "\n",
      "        temp_dict['feature_vector'] = BagOfWords().construct_feature(\n",
      "                    img, kmeans, centroids)\n",
      "\n",
      "        if temp_dict['feature_vector'] is not None:\n",
      "            feature_map.append(temp_dict)\n",
      "\n",
      "    return feature_map\n",
      "\n",
      "# Extract SIFT features\n",
      "def compute_sift_features(img, keypoints):\n",
      "    if img is None:\n",
      "        raise TypeError('Invalid input image')\n",
      "\n",
      "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
      "    keypoints, descriptors = cv2.xfeatures2d.SIFT_create().compute(img_gray, keypoints)\n",
      "    return keypoints, descriptors\n",
      "\n",
      "# Resize the shorter dimension to 'new_size' \n",
      "# while maintaining the aspect ratio\n",
      "def resize_image(input_img, new_size):\n",
      "    h, w = input_img.shape[:2]\n",
      "    scaling_factor = new_size / float(h)\n",
      "\n",
      "    if w < h:\n",
      "        scaling_factor = new_size / float(w)\n",
      "\n",
      "    new_shape = (int(w * scaling_factor), int(h * scaling_factor))\n",
      "    return cv2.resize(input_img, new_shape) \n",
      "\n",
      "if __name__=='__main__':\n",
      "    args = build_arg_parser().parse_args()\n",
      "    data_folder = args.data_folder\n",
      "    scaling_size = args.scaling_size\n",
      "    \n",
      "    # Load the training data\n",
      "    training_data = load_training_data(data_folder)\n",
      "\n",
      "    # Build the visual codebook\n",
      "    print(\"====== Building visual codebook ======\")\n",
      "    kmeans, centroids = FeatureBuilder().get_codewords(training_data, scaling_size)\n",
      "    if args.codebook_file:\n",
      "        with open(args.codebook_file, 'w') as f:\n",
      "            pickle.dump((kmeans, centroids), f)\n",
      "    \n",
      "    # Extract features from input images\n",
      "    print(\"\\n====== Building the feature map ======\")\n",
      "    feature_map = get_feature_map(training_data, kmeans, centroids, scaling_size)\n",
      "    if args.feature_map_file:\n",
      "        with open(args.feature_map_file, 'w') as f:\n",
      "            pickle.dump(feature_map, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
