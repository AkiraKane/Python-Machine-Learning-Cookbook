{
 "metadata": {},
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer  \n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from gensim import models, corpora\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "# Load input data\n",
      "def load_data(input_file):\n",
      "    data = []\n",
      "    with open(input_file, 'r') as f:\n",
      "        for line in f.readlines():\n",
      "            data.append(line[:-1])\n",
      "\n",
      "    return data\n",
      "\n",
      "# Class to preprocess text\n",
      "class Preprocessor(object):\n",
      "    # Initialize various operators\n",
      "    def __init__(self):\n",
      "        # Create a regular expression tokenizer\n",
      "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
      "\n",
      "        # get the list of stop words \n",
      "        self.stop_words_english = stopwords.words('english')\n",
      "\n",
      "        # Create a Snowball stemmer \n",
      "        self.stemmer = SnowballStemmer('english')\n",
      "        \n",
      "    # Tokenizing, stop word removal, and stemming\n",
      "    def process(self, input_text):\n",
      "        # Tokenize the string\n",
      "        tokens = self.tokenizer.tokenize(input_text.lower())\n",
      "\n",
      "        # Remove the stop words \n",
      "        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]\n",
      "        \n",
      "        # Perform stemming on the tokens \n",
      "        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]\n",
      "\n",
      "        return tokens_stemmed\n",
      "    \n",
      "if __name__=='__main__':\n",
      "    # File containing linewise input data \n",
      "    input_file = 'data_topic_modeling.txt'\n",
      "\n",
      "    # Load data\n",
      "    data = load_data(input_file)\n",
      "\n",
      "    # Create a preprocessor object\n",
      "    preprocessor = Preprocessor()\n",
      "\n",
      "    # Create a list for processed documents\n",
      "    processed_tokens = [preprocessor.process(x) for x in data]\n",
      "\n",
      "    # Create a dictionary based on the tokenized documents\n",
      "    dict_tokens = corpora.Dictionary(processed_tokens)\n",
      "        \n",
      "    # Create a document-term matrix\n",
      "    corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n",
      "\n",
      "    # Generate the LDA model based on the corpus we just created\n",
      "    num_topics = 2\n",
      "    num_words = 4\n",
      "    ldamodel = models.ldamodel.LdaModel(corpus, \n",
      "            num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
      "\n",
      "    print(\"\\nMost contributing words to the topics:\")\n",
      "    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
      "        print(\"\\nTopic\", item[0], \"==>\", item[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
