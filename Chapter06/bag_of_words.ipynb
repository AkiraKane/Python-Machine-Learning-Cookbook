{
 "metadata": {},
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from nltk.corpus import brown\n",
      "from chunking import splitter\n",
      "\n",
      "if __name__=='__main__':\n",
      "    # Read the data from the Brown corpus\n",
      "    data = ' '.join(brown.words()[:10000])\n",
      "\n",
      "    # Number of words in each chunk \n",
      "    num_words = 2000\n",
      "\n",
      "    chunks = []\n",
      "    counter = 0\n",
      "\n",
      "    text_chunks = splitter(data, num_words)\n",
      "\n",
      "    for text in text_chunks:\n",
      "        chunk = {'index': counter, 'text': text}\n",
      "        chunks.append(chunk)\n",
      "        counter += 1\n",
      "\n",
      "    # Extract document term matrix\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "    vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
      "    doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
      "\n",
      "    vocab = np.array(vectorizer.get_feature_names())\n",
      "    print(\"\\nVocabulary:\")\n",
      "    print(vocab)\n",
      "\n",
      "    print(\"\\nDocument term matrix:\")\n",
      "    chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4']\n",
      "    formatted_row = '{:>12}' * (len(chunk_names) + 1)\n",
      "    print('\\n', formatted_row.format('Word', *chunk_names), '\\n')\n",
      "    for word, item in zip(vocab, doc_term_matrix.T):\n",
      "        # 'item' is a 'csr_matrix' data structure\n",
      "        output = [str(x) for x in item.data]\n",
      "        print(formatted_row.format(word, *output))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
