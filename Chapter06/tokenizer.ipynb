{
 "metadata": {},
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\"\n",
      "\n",
      "# Sentence tokenization\n",
      "from nltk.tokenize import sent_tokenize\n",
      "\n",
      "sent_tokenize_list = sent_tokenize(text)\n",
      "print(\"\\nSentence tokenizer:\")\n",
      "print(sent_tokenize_list)\n",
      "\n",
      "# Create a new word tokenizer\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "print(\"\\nWord tokenizer:\")\n",
      "print(word_tokenize(text))\n",
      "\n",
      "# Create a new punkt word tokenizer\n",
      "from nltk.tokenize import PunktWordTokenizer\n",
      "\n",
      "punkt_word_tokenizer = PunktWordTokenizer()\n",
      "print(\"\\nPunkt word tokenizer:\")\n",
      "print(punkt_word_tokenizer.tokenize(text))\n",
      "\n",
      "# Create a new WordPunct tokenizer\n",
      "from nltk.tokenize import WordPunctTokenizer\n",
      "\n",
      "word_punct_tokenizer = WordPunctTokenizer()\n",
      "print(\"\\nWord punct tokenizer:\")\n",
      "print(word_punct_tokenizer.tokenize(text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
