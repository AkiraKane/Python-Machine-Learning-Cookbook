{
 "metadata": {},
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from nltk.corpus import brown\n",
      "\n",
      "# Split a text into chunks \n",
      "def splitter(data, num_words):\n",
      "    words = data.split(' ')\n",
      "    output = []\n",
      "\n",
      "    cur_count = 0\n",
      "    cur_words = []\n",
      "    for word in words:\n",
      "        cur_words.append(word)\n",
      "        cur_count += 1\n",
      "        if cur_count == num_words:\n",
      "            output.append(' '.join(cur_words))\n",
      "            cur_words = []\n",
      "            cur_count = 0\n",
      "\n",
      "    output.append(' '.join(cur_words) )\n",
      "\n",
      "    return output \n",
      "\n",
      "if __name__=='__main__':\n",
      "    # Read the data from the Brown corpus\n",
      "    data = ' '.join(brown.words()[:10000])\n",
      "\n",
      "    # Number of words in each chunk \n",
      "    num_words = 1700\n",
      "\n",
      "    chunks = []\n",
      "    counter = 0\n",
      "\n",
      "    text_chunks = splitter(data, num_words)\n",
      "\n",
      "    print(\"Number of text chunks =\", len(text_chunks))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
